{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nmhlog/Naufal Disk/Thesis Experiment/thesis/thesis-lib/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, numpy as np, torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "\n",
    "def splitPointCloud(cloud, size=50.0, stride=25):\n",
    "    limitMax = np.amax(cloud[:, 0:3], axis=0)\n",
    "    width = int(np.ceil((limitMax[0] - size) / stride)) + 1\n",
    "    depth = int(np.ceil((limitMax[1] - size) / stride)) + 1\n",
    "    cells = [(x * stride, y * stride) for x in range(width) for y in range(depth)]\n",
    "    blocks = []\n",
    "    for (x, y) in cells:\n",
    "        xcond = (cloud[:, 0] <= x + size) & (cloud[:, 0] >= x)\n",
    "        ycond = (cloud[:, 1] <= y + size) & (cloud[:, 1] >= y)\n",
    "        cond  = xcond & ycond\n",
    "        block = cloud[cond, :]\n",
    "        blocks.append(block)\n",
    "    return blocks\n",
    "\n",
    "def getFiles(files,fileSplit):\n",
    "    res = []\n",
    "    for filePath in files:\n",
    "        name = os.path.basename(filePath)\n",
    "        num = name[:2] if name[:2].isdigit() else name[:1]\n",
    "        if int(num) in fileSplit:\n",
    "            res.append(filePath)\n",
    "    return res\n",
    "\n",
    "def dataAug(file,semanticKeep):\n",
    "    points = pd.read_csv(file, header = None,delimiter=\" \").values\n",
    "    angle = random.randint(1, 359)\n",
    "    angleRadians = math.radians(angle)\n",
    "    rotationMatrix = np.array([[math.cos(angleRadians), -math.sin(angleRadians),0],[math.sin(angleRadians),math.cos(angleRadians), 0],[0,0,1]])\n",
    "    points[:,:3] = points[:,:3].dot(rotationMatrix)\n",
    "    pointsKept = points[np.in1d(points[:,6], semanticKeep)]\n",
    "    return pointsKept\n",
    "\n",
    "def preparePthFiles(files, split, outPutFolder, AugTimes=0):\n",
    "    ### save the coordinates so that we can merge the data to a single scene after segmentation for visualization\n",
    "    outJsonPath = os.path.join(outPutFolder, 'coordShift.json')\n",
    "    coordShift = {}\n",
    "    ### used to increase z range if it is smaller than this, over come the issue where spconv may crash for voxlization.\n",
    "    zThreshold = 6\n",
    "\n",
    "    # Map relevant classes to {1,...,14}, and ignored classes to -100\n",
    "    remapper = np.ones(150) * (-100)\n",
    "    for i, x in enumerate([0, 1, 2, 3]):\n",
    "        remapper[x] = i\n",
    "    # Map instance to -100 based on selected semantic (change a semantic to -100 if you want to ignore it for instance)\n",
    "    remapper_disableInstanceBySemantic = np.ones(150) * (-100)\n",
    "    for i, x in enumerate([-100, 1]):\n",
    "        remapper_disableInstanceBySemantic[x] = i\n",
    "\n",
    "    ### only augment data for these classes\n",
    "    semanticKeep = [0, 2, 3]\n",
    "\n",
    "    counter = 0\n",
    "    for file in files:\n",
    "\n",
    "        for AugTime in range(AugTimes+1):\n",
    "            if AugTime == 0:\n",
    "                points = pd.read_csv(file, header=None,delimiter=\" \").values\n",
    "            else:\n",
    "                points = dataAug(file,semanticKeep)\n",
    "            name = os.path.basename(file).strip('.txt')+'_%d'%AugTime\n",
    "\n",
    "            if split != 'test':\n",
    "                coordShift['globalShift'] = list(points[:, :3].min(0))\n",
    "            points[:, :3] = points[:, :3] - points[:, :3].min(0)\n",
    "\n",
    "            blocks = splitPointCloud(points, size=50, stride=50)\n",
    "            for blockNum, block in enumerate(blocks):\n",
    "                if (len(block) > 10000):\n",
    "                    outFilePath = os.path.join(outPutFolder, name + str(blockNum) + '_inst_nostuff.pth')\n",
    "                    if (block[:, 2].max(0) - block[:, 2].min(0) < zThreshold):\n",
    "                        block = np.append(block, [[block[:, 0].mean(0), block[:, 1].mean(0),\n",
    "                                                   block[:, 2].max(0) + (\n",
    "                                                               zThreshold - (block[:, 2].max(0) - block[:, 2].min(0))),\n",
    "                                                   block[:, 3].mean(0), block[:, 4].mean(0), block[:, 5].mean(0),\n",
    "                                                   -100, -100]], axis=0)\n",
    "                        print(\"range z is smaller than threshold \")\n",
    "                        print(name + str(blockNum) + '_inst_nostuff')\n",
    "                    if split != 'test':\n",
    "                        outFileName = name + str(blockNum) + '_inst_nostuff'\n",
    "                        coordShift[outFileName] = list(block[:, :3].mean(0))\n",
    "                    coords = np.ascontiguousarray(block[:, :3] - block[:, :3].mean(0))\n",
    "\n",
    "                    # coords = block[:, :3]\n",
    "                    colors = np.ascontiguousarray(block[:, 3:6]) \n",
    "                    # colors = np.ascontiguousarray(block[:, 3:6]) / 127.5 - 1\n",
    "\n",
    "                    coords = np.float32(coords)\n",
    "                    colors = np.float32(colors)\n",
    "                    if split != 'test':\n",
    "                        sem_labels = np.ascontiguousarray(block[:, -2])\n",
    "                        sem_labels = sem_labels.astype(np.int32)\n",
    "                        sem_labels = remapper[np.array(sem_labels)]\n",
    "\n",
    "                        instance_labels = np.ascontiguousarray(block[:, -1])\n",
    "                        instance_labels = instance_labels.astype(np.float32)\n",
    "\n",
    "                        disableInstanceBySemantic_labels = np.ascontiguousarray(block[:, -2])\n",
    "                        disableInstanceBySemantic_labels = disableInstanceBySemantic_labels.astype(np.int32)\n",
    "                        disableInstanceBySemantic_labels = remapper_disableInstanceBySemantic[\n",
    "                            np.array(disableInstanceBySemantic_labels)]\n",
    "                        instance_labels = np.where(disableInstanceBySemantic_labels == -100, -100, instance_labels)\n",
    "\n",
    "                        # map instance from 0.\n",
    "                        # [1:] because there are -100\n",
    "                        uniqueInstances = (np.unique(instance_labels))[1:].astype(np.int32)\n",
    "                        remapper_instance = np.ones(50000) * (-100)\n",
    "                        for i, j in enumerate(uniqueInstances):\n",
    "                            remapper_instance[j] = i\n",
    "\n",
    "                        instance_labels = remapper_instance[instance_labels.astype(np.int32)]\n",
    "\n",
    "                        uniqueSemantics = (np.unique(sem_labels))[1:].astype(np.int32)\n",
    "\n",
    "                        if split == 'train' and (\n",
    "                                len(uniqueInstances) < 10 or (len(uniqueSemantics) >= (len(uniqueInstances) - 2))):\n",
    "                            print(\"unique insance: %d\" % len(uniqueInstances))\n",
    "                            print(\"unique semantic: %d\" % len(uniqueSemantics))\n",
    "                            print()\n",
    "                            counter += 1\n",
    "                        else:\n",
    "                            # torch.save((coords, colors, sem_labels, instance_labels), outFilePath)\n",
    "                            __buff= np.hstack((coords,\\\n",
    "                                colors, \n",
    "                                sem_labels.reshape([len(sem_labels),1]),\n",
    "                                instance_labels.reshape([len(instance_labels),1])\n",
    "                                ))\n",
    "                            np.savetxt(outFilePath[:-4]+\".txt\",__buff)\n",
    "                            ### save text file for each pth file\n",
    "                            # outFilePath = os.path.join(outPutFolder,name+str(blockNum)+'.txt')\n",
    "                            # outFile = open(outFilePath,'w')\n",
    "                            # for i in range(len(coords)):\n",
    "                            #     outFile.write(\"%f,%f,%f,%f,%f,%f,%d,%d\\n\" %(coords[i][0],coords[i][1],coords[i][2],\n",
    "                            #                                                 colors[i][0],colors[i][1],colors[i][2],\n",
    "                            #                                                 sem_labels[i],instance_labels[i]))\n",
    "                    else:\n",
    "                        torch.save((coords, colors), outFilePath)\n",
    "                        # # save text file for each pth file\n",
    "                        # outFilePath = os.path.join(outPutFolder,name+str(blockNum)+'.txt')\n",
    "                        # outFile = open(outFilePath,'w')\n",
    "                        # for i in range(len(coords)):\n",
    "                        #     outFile.write(\"%f,%f,%f,%f,%f,%f\\n\" %(coords[i][0],coords[i][1],coords[i][2],\n",
    "                        #                                                 colors[i][0],colors[i][1],colors[i][2]))\n",
    "    print(\"Total skipped file :%d\" % counter)\n",
    "    json.dump(coordShift, open(outJsonPath, 'w'))\n",
    "\n",
    "def prepareInstGt(valOutDir, val_gtFolder,semantic_label_idxs):\n",
    "    valFilesPth = sorted(glob.glob('{}/*_inst_nostuff.pth'.format(valOutDir)))\n",
    "    blocks = [torch.load(i) for i in valFilesPth]\n",
    "\n",
    "    for i in range(len(blocks)):\n",
    "        xyz, rgb, label, instance_label = blocks[i]  # label 0~19 -100;  instance_label 0~instance_num-1 -100\n",
    "        scene_name = os.path.basename(valFilesPth[i]).strip('.pth')\n",
    "        print('{}/{} {}'.format(i + 1, len(blocks), scene_name))\n",
    "\n",
    "        instance_label_new = np.zeros(instance_label.shape,\n",
    "                                      dtype=np.int32)  # 0 for unannotated, xx00y: x for semantic_label, y for inst_id (1~instance_num)\n",
    "\n",
    "        instance_num = int(instance_label.max()) + 1\n",
    "        for inst_id in range(instance_num):\n",
    "            instance_mask = np.where(instance_label == inst_id)[0]\n",
    "            sem_id = int(label[instance_mask[0]])\n",
    "            if (sem_id == -100): sem_id = 0\n",
    "            semantic_label = semantic_label_idxs[sem_id]\n",
    "            instance_label_new[instance_mask] = semantic_label * 1000 + inst_id + 1\n",
    "\n",
    "        np.savetxt(os.path.join(val_gtFolder, scene_name + '.txt'), instance_label_new, fmt='%d')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    data_folder = os.path.join('data_annotated_new')\n",
    "    filesOri = sorted(glob.glob(data_folder + '/*.txt'))\n",
    "\n",
    "    trainSplit = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
    "    trainFiles = getFiles(filesOri,trainSplit)\n",
    "    split = 'train'\n",
    "    trainOutDir = os.path.join(data_folder,split)\n",
    "    os.makedirs(trainOutDir,exist_ok=True)\n",
    "    preparePthFiles(trainFiles, split, trainOutDir, AugTimes= 1)\n",
    "\n",
    "    # valSplit = [5, 10, 15, 20, 25]\n",
    "    # split = 'val'\n",
    "    # valFiles = getFiles(filesOri, valSplit)\n",
    "    # valOutDir = os.path.join(data_folder,split)\n",
    "    # os.makedirs(valOutDir,exist_ok=True)\n",
    "    # preparePthFiles(valFiles, split, valOutDir)\n",
    "\n",
    "    # semantic_label_idxs = [0, 1, 2, 3]\n",
    "    # semantic_label_names = ['ground', 'building', 'vegetation', 'other']\n",
    "    # val_gtFolder = os.path.join(data_folder,'val_gt')\n",
    "    # os.makedirs(val_gtFolder,exist_ok=True)\n",
    "    # prepareInstGt(valOutDir, val_gtFolder, semantic_label_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = pd.read_csv(\"data_annotated_new/1_points_GTv3.txt\", header=None,delimiter=\" \").values\n",
    "name = os.path.basename(\"data_annotated_new/1_points_GTv3.txt\").strip('.txt')\n",
    "# coordShift['globalShift'] = list(points[:, :3].min(0))\n",
    "points[:, :3] = points[:, :3] - points[:, :3].min(0)\n",
    "\n",
    "blocks = splitPointCloud(points, size=50, stride=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/media/nmhlog/Naufal Disk/Thesis Experiment/thesis/preprocessing.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/nmhlog/Naufal%20Disk/Thesis%20Experiment/thesis/preprocessing.ipynb#ch0000003?line=0'>1</a>\u001b[0m blocks\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "blocks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "85938aeac5d3fc666b9772978340405759206969cda94d8472987c4123c8d935"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('thesis-lib': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
