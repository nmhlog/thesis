{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "\n",
    "data_folder = os.path.join(\"Synthetic_v3_InstanceSegmentation\\\\train\")\n",
    "files = sorted(glob.glob(data_folder + '\\\\*.pth'))\n",
    "numclass = 15\n",
    "semanticIDs = []\n",
    "for i in range(numclass):\n",
    "    semanticIDs.append(i)\n",
    "\n",
    "class_numpoint_mean_dict = {}\n",
    "class_radius_mean = {}\n",
    "for semanticID in semanticIDs:\n",
    "    class_numpoint_mean_dict[semanticID] = []\n",
    "    class_radius_mean[semanticID] = []\n",
    "num_points_semantic = np.array([0 for i in range(numclass)])\n",
    "\n",
    "for file in files:\n",
    "    coords, colors, sem_labels, instance_labels = torch.load(file)\n",
    "    points = np.concatenate((coords,colors,np.expand_dims(sem_labels.astype(int),axis=1),np.expand_dims(instance_labels.astype(int),axis=1)),axis=1)\n",
    "    for semanticID in semanticIDs:\n",
    "        singleSemantic = points[np.where(points[:,6]==semanticID)]\n",
    "        uniqueInstances, counts = np.unique(singleSemantic[:,7], return_counts=True)\n",
    "        for count in counts:\n",
    "            class_numpoint_mean_dict[semanticID].append(count)\n",
    "        allRadius = []\n",
    "        for uniqueInstance in uniqueInstances:\n",
    "            eachInstance = singleSemantic[np.where(singleSemantic[:,7]==uniqueInstance)]\n",
    "            radius = (np.max(eachInstance,axis=0) - np.min(eachInstance,axis=0))/2\n",
    "            radius = math.sqrt(radius[0]**2 + radius[1]**2 + radius[2]**2)\n",
    "            class_radius_mean[semanticID].append(radius)\n",
    "\n",
    "    uniqueSemantic,semanticCount = np.unique(points[:,6],return_counts=True)\n",
    "    uniqueSemanticCount = np.array([0 for i in range(numclass)])\n",
    "    uniqueSemantic = uniqueSemantic.astype(int)\n",
    "    indexOf100 = np.where(uniqueSemantic == -100)\n",
    "    semanticCount = np.delete(semanticCount, indexOf100)\n",
    "    uniqueSemantic = np.delete(uniqueSemantic, indexOf100)\n",
    "    uniqueSemanticCount[uniqueSemantic] = semanticCount\n",
    "    num_points_semantic += uniqueSemanticCount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Synthetic_v3_InstanceSegmentation\\\\10_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\11_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\12_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\13_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\14_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\15_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\16_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\17_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\18_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\19_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\1_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\20_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\21_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\22_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\23_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\24_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\25_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\2_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\3_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\4_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\5_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\6_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\7_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\8_points_GTv3.txt',\n",
       " 'Synthetic_v3_InstanceSegmentation\\\\9_points_GTv3.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([86451935, 55442153,   219780,   440822, 36336088,  2553947,\n",
       "        2031488,   379779,   340695,   173834,   576037,   782733,\n",
       "         217991,   992942,  3237132])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_points_semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_numpoint_mean_list = []\n",
    "# class_radius_mean_list = []\n",
    "# for semanticID in semanticIDs:\n",
    "#     class_numpoint_mean_list.append(sum(class_numpoint_mean_dict[semanticID])*1.0/len(class_numpoint_mean_dict[semanticID]))\n",
    "#     class_radius_mean_list.append(sum(class_radius_mean[semanticID])/len(class_radius_mean[semanticID]))\n",
    "\n",
    "# print (\"Using the printed list in hierarchical_aggregation.cpp for class_numpoint_mean_dict: \")\n",
    "# print ([1.0]+[float(\"{0:0.0f}\".format(i)) for i in class_numpoint_mean_list][1:], sep=',')\n",
    "# print (\"Using the printed list in hierarchical_aggregation.cu for class_radius_mean: \")\n",
    "# print ([1.0]+[float(\"{0:0.2f}\".format(i)) for i in class_radius_mean_list][1:], sep='')\n",
    "\n",
    "### make ground to 1 the make building to 1\n",
    "maxSemantic = np.max(num_points_semantic)\n",
    "num_points_semantic_buff = maxSemantic/num_points_semantic\n",
    "# num_points_semantic_buff = num_points_semantic_buff/num_points_semantic_buff[1]\n",
    "# print (\"Using the printed list in hais_run_stpls3d.yaml for class_weight\")\n",
    "# print ([1.0,1.0]+[float(\"{0:0.2f}\".format(i)) for i in num_points_semantic][2:], sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.        ,   1.55931778, 393.35669761, 196.11529143,\n",
       "         2.37923067,  33.85032462,  42.55596637, 227.63748127,\n",
       "       253.75169873, 497.32466031, 150.08052434, 110.44881843,\n",
       "       396.58488194,  87.06645   ,  26.70633604])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_points_semantic_buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.6413061 ,   1.        , 252.26204841, 125.76993208,\n",
       "         1.52581513,  21.70841956,  27.29140069, 145.98530461,\n",
       "       162.73251148, 318.93733677,  96.24755528,  70.83150065,\n",
       "       254.33230271,  55.83624522,  17.12693613])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_points_semantic_buff/num_points_semantic_buff[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a15ac88d0b5eb35656cb2c4f0309efe5efc3230ed0b4dd02c2a553aad714dbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
