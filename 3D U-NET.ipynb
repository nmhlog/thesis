{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "KeyboardInterrupt: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/media/nmhlog/Naufal Disk/Thesis Experiment/github/thesis/3D U-NET.ipynb Cell 1'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/nmhlog/Naufal%20Disk/Thesis%20Experiment/github/thesis/3D%20U-NET.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/nmhlog/Naufal%20Disk/Thesis%20Experiment/github/thesis/3D%20U-NET.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/nmhlog/Naufal%20Disk/Thesis%20Experiment/github/thesis/3D%20U-NET.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspconv\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/__init__.py:196\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    <a href='file:///home/nmhlog/.local/lib/python3.8/site-packages/torch/__init__.py?line=193'>194</a>\u001b[0m     \u001b[39mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    <a href='file:///home/nmhlog/.local/lib/python3.8/site-packages/torch/__init__.py?line=194'>195</a>\u001b[0m         _load_global_deps()\n\u001b[0;32m--> <a href='file:///home/nmhlog/.local/lib/python3.8/site-packages/torch/__init__.py?line=195'>196</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    <a href='file:///home/nmhlog/.local/lib/python3.8/site-packages/torch/__init__.py?line=197'>198</a>\u001b[0m \u001b[39m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/nmhlog/.local/lib/python3.8/site-packages/torch/__init__.py?line=198'>199</a>\u001b[0m \u001b[39m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/nmhlog/.local/lib/python3.8/site-packages/torch/__init__.py?line=199'>200</a>\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: KeyboardInterrupt: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import spconv\n",
    "from spconv.modules import SparseModule\n",
    "import functools\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from lib.hais_ops.functions import hais_ops\n",
    "from util import utils\n",
    "\n",
    "\n",
    "class ResidualBlock(SparseModule):\n",
    "    def __init__(self, in_channels, out_channels, norm_fn, indice_key=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.i_branch = spconv.SparseSequential(\n",
    "                nn.Identity()\n",
    "            )\n",
    "        else:\n",
    "            self.i_branch = spconv.SparseSequential(\n",
    "                spconv.SubMConv3d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "            )\n",
    "\n",
    "        self.conv_branch = spconv.SparseSequential(\n",
    "            norm_fn(in_channels),\n",
    "            nn.ReLU(),\n",
    "            spconv.SubMConv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False, indice_key=indice_key),\n",
    "            norm_fn(out_channels),\n",
    "            nn.ReLU(),\n",
    "            spconv.SubMConv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=False, indice_key=indice_key)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        identity = spconv.SparseConvTensor(input.features, input.indices, input.spatial_shape, input.batch_size)\n",
    "        output = self.conv_branch(input)\n",
    "        output.features += self.i_branch(identity).features\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class VGGBlock(SparseModule):\n",
    "    def __init__(self, in_channels, out_channels, norm_fn, indice_key=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = spconv.SparseSequential(\n",
    "            norm_fn(in_channels),\n",
    "            nn.ReLU(),\n",
    "            spconv.SubMConv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False, indice_key=indice_key)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv_layers(input)\n",
    "\n",
    "\n",
    "class UBlock(nn.Module):\n",
    "    def __init__(self, nPlanes, norm_fn, block_reps, block, indice_key_id=1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.nPlanes = nPlanes\n",
    "\n",
    "        blocks = {'block{}'.format(i): block(nPlanes[0], nPlanes[0], norm_fn, indice_key='subm{}'.format(indice_key_id)) for i in range(block_reps)}\n",
    "        blocks = OrderedDict(blocks)\n",
    "        self.blocks = spconv.SparseSequential(blocks)\n",
    "\n",
    "        if len(nPlanes) > 1:\n",
    "            self.conv = spconv.SparseSequential(\n",
    "                norm_fn(nPlanes[0]),\n",
    "                nn.ReLU(),                                   \n",
    "                spconv.SparseConv3d(nPlanes[0], nPlanes[1], kernel_size=2, stride=2, bias=False, indice_key='spconv{}'.format(indice_key_id))\n",
    "            )\n",
    "\n",
    "            self.u = UBlock(nPlanes[1:], norm_fn, block_reps, block, indice_key_id=indice_key_id+1)\n",
    "\n",
    "            self.deconv = spconv.SparseSequential(\n",
    "                norm_fn(nPlanes[1]),\n",
    "                nn.ReLU(),                                             \n",
    "                spconv.SparseInverseConv3d(nPlanes[1], nPlanes[0], kernel_size=2, bias=False, indice_key='spconv{}'.format(indice_key_id))\n",
    "            )\n",
    "\n",
    "            blocks_tail = {}\n",
    "            for i in range(block_reps):\n",
    "                blocks_tail['block{}'.format(i)] = block(nPlanes[0] * (2 - i), nPlanes[0], norm_fn, indice_key='subm{}'.format(indice_key_id))\n",
    "            blocks_tail = OrderedDict(blocks_tail)\n",
    "            self.blocks_tail = spconv.SparseSequential(blocks_tail)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.blocks(input)\n",
    "        identity = spconv.SparseConvTensor(output.features, output.indices, output.spatial_shape, output.batch_size)\n",
    "        if len(self.nPlanes) > 1:\n",
    "            output_decoder = self.conv(output)\n",
    "            output_decoder = self.u(output_decoder)\n",
    "            output_decoder = self.deconv(output_decoder)\n",
    "            output.features = torch.cat((identity.features, output_decoder.features), dim=1)\n",
    "            output = self.blocks_tail(output)\n",
    "        return output\n",
    "\n",
    "class HAIS(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        input_c = cfg.input_channel\n",
    "        width = cfg.width\n",
    "        classes = cfg.classes\n",
    "        block_reps = cfg.block_reps\n",
    "        block_residual = cfg.block_residual\n",
    "\n",
    "        self.point_aggr_radius = cfg.point_aggr_radius\n",
    "        self.cluster_shift_meanActive = cfg.cluster_shift_meanActive\n",
    "\n",
    "        self.score_scale = cfg.score_scale\n",
    "        self.score_fullscale = cfg.score_fullscale\n",
    "        self.score_mode = cfg.score_mode\n",
    "\n",
    "        self.prepare_epochs = cfg.prepare_epochs\n",
    "        self.pretrain_path = cfg.pretrain_path\n",
    "        self.pretrain_module = cfg.pretrain_module\n",
    "        self.fix_module = cfg.fix_module\n",
    "        \n",
    "\n",
    "        norm_fn = functools.partial(nn.BatchNorm1d, eps=1e-4, momentum=0.1)\n",
    "\n",
    "        if block_residual:\n",
    "            block = ResidualBlock\n",
    "        else:\n",
    "            block = VGGBlock\n",
    "\n",
    "        if cfg.use_coords:\n",
    "            input_c += 3\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # backbone\n",
    "        self.input_conv = spconv.SparseSequential(\n",
    "            spconv.SubMConv3d(input_c, width, kernel_size=3, padding=1, bias=False, indice_key='subm1')\n",
    "        )\n",
    "        self.unet = UBlock([width, 2*width, 3*width, 4*width, 5*width, 6*width, 7*width], norm_fn, block_reps, block, indice_key_id=1)\n",
    "        self.output_layer = spconv.SparseSequential(\n",
    "            norm_fn(width),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # semantic segmentation branch\n",
    "        self.semantic_linear = nn.Sequential(\n",
    "            nn.Linear(width, width, bias=True),\n",
    "            norm_fn(width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(width, classes)\n",
    "        )\n",
    "\n",
    "        # center shift vector branch\n",
    "        self.offset_linear = nn.Sequential(\n",
    "            nn.Linear(width, width, bias=True),\n",
    "            norm_fn(width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(width, 3, bias=True)\n",
    "        )\n",
    "\n",
    "        # intra-instance network\n",
    "        self.intra_ins_unet = UBlock([width, 2*width], norm_fn, 2, block, indice_key_id=11)\n",
    "        self.intra_ins_outputlayer = spconv.SparseSequential(\n",
    "            norm_fn(width),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # proposal score\n",
    "        self.score_linear = nn.Linear(width, 1)\n",
    "\n",
    "        # proposal mask\n",
    "        self.mask_linear = nn.Sequential(\n",
    "                nn.Linear(width, width),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(width, 1))\n",
    "\n",
    "        self.apply(self.set_bn_init)\n",
    "\n",
    "\n",
    "        # fix module\n",
    "        module_map = {'input_conv': self.input_conv, 'unet': self.unet, 'output_layer': self.output_layer,\n",
    "                      'semantic_linear': self.semantic_linear, 'offset_linear': self.offset_linear,\n",
    "                      'intra_ins_unet': self.intra_ins_unet, 'intra_ins_outputlayer': self.intra_ins_outputlayer, \n",
    "                      'score_linear': self.score_linear, 'mask_linear': self.mask_linear}\n",
    "        # for m in self.fix_module:\n",
    "        #     mod = module_map[m]\n",
    "        #     for param in mod.parameters():\n",
    "        #         param.requires_grad = False\n",
    "\n",
    "        # # load pretrain weights\n",
    "        # if self.pretrain_path is not None:\n",
    "        #     pretrain_dict = torch.load(self.pretrain_path)\n",
    "        #     for m in self.pretrain_module:\n",
    "        #         print(\"Load pretrained \" + m + \": %d/%d\" % utils.load_model_param(module_map[m], pretrain_dict, prefix=m))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def set_bn_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('BatchNorm') != -1:\n",
    "            m.weight.data.fill_(1.0)\n",
    "            m.bias.data.fill_(0.0)\n",
    "\n",
    "    \"\"\"\n",
    "    def clusters_voxelization(self, clusters_idx, clusters_offset, feats, coords, fullscale, scale, mode):\n",
    "        '''\n",
    "        :param clusters_idx: (SumNPoint, 2), int, [:, 0] for cluster_id, [:, 1] for corresponding point idxs in N, cpu\n",
    "        :param clusters_offset: (nCluster + 1), int, cpu\n",
    "        :param feats: (N, C), float, cuda\n",
    "        :param coords: (N, 3), float, cuda\n",
    "        :return:\n",
    "        '''\n",
    "        c_idxs = clusters_idx[:, 1].cuda()\n",
    "        clusters_feats = feats[c_idxs.long()]\n",
    "        clusters_coords = coords[c_idxs.long()]\n",
    "\n",
    "        clusters_coords_mean = hais_ops.sec_mean(clusters_coords, clusters_offset.cuda())  # (nCluster, 3), float\n",
    "        clusters_coords_mean = torch.index_select(clusters_coords_mean, 0, clusters_idx[:, 0].cuda().long())  # (sumNPoint, 3), float\n",
    "        clusters_coords -= clusters_coords_mean\n",
    "\n",
    "        clusters_coords_min = hais_ops.sec_min(clusters_coords, clusters_offset.cuda())  # (nCluster, 3), float\n",
    "        clusters_coords_max = hais_ops.sec_max(clusters_coords, clusters_offset.cuda())  # (nCluster, 3), float\n",
    "        # print (clusters_coords_min)\n",
    "        # print (clusters_coords_max)\n",
    "        clusters_scale = 1 / ((clusters_coords_max - clusters_coords_min) / fullscale).max(1)[0] - 0.01  # (nCluster), float\n",
    "        # print (clusters_scale)\n",
    "        clusters_scale = torch.clamp(clusters_scale, min=None, max=scale)\n",
    "        # print (clusters_scale)\n",
    "\n",
    "        min_xyz = clusters_coords_min * clusters_scale.unsqueeze(-1)  # (nCluster, 3), float\n",
    "        max_xyz = clusters_coords_max * clusters_scale.unsqueeze(-1)\n",
    "\n",
    "        clusters_scale = torch.index_select(clusters_scale, 0, clusters_idx[:, 0].cuda().long())\n",
    "\n",
    "        clusters_coords = clusters_coords * clusters_scale.unsqueeze(-1)\n",
    "\n",
    "        range = max_xyz - min_xyz\n",
    "        offset = - min_xyz + torch.clamp(fullscale - range - 0.001, min=0) * torch.rand(3).cuda() + torch.clamp(fullscale - range + 0.001, max=0) * torch.rand(3).cuda()\n",
    "        offset = torch.index_select(offset, 0, clusters_idx[:, 0].cuda().long())\n",
    "        clusters_coords += offset\n",
    "        assert clusters_coords.shape.numel() == ((clusters_coords >= 0) * (clusters_coords < fullscale)).sum()\n",
    "\n",
    "        clusters_coords = clusters_coords.long()\n",
    "        clusters_coords = torch.cat([clusters_idx[:, 0].view(-1, 1).long(), clusters_coords.cpu()], 1)  # (sumNPoint, 1 + 3)\n",
    "\n",
    "        out_coords, inp_map, out_map = hais_ops.voxelization_idx(clusters_coords, int(clusters_idx[-1, 0]) + 1, mode)\n",
    "        # output_coords: M * (1 + 3) long\n",
    "        # input_map: sumNPoint int\n",
    "        # output_map: M * (maxActive + 1) int\n",
    "\n",
    "        out_feats = hais_ops.voxelization(clusters_feats, out_map.cuda(), mode)  # (M, C), float, cuda\n",
    "\n",
    "        spatial_shape = [fullscale] * 3\n",
    "        voxelization_feats = spconv.SparseConvTensor(out_feats, out_coords.int().cuda(), spatial_shape, int(clusters_idx[-1, 0]) + 1)\n",
    "\n",
    "        return voxelization_feats, inp_map\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, input, input_map, coords, batch_idxs, batch_offsets, epoch, training_mode):\n",
    "        '''\n",
    "        :param input_map: (N), int, cuda\n",
    "        :param coords: (N, 3), float, cuda\n",
    "        :param batch_idxs: (N), int, cuda\n",
    "        :param batch_offsets: (B + 1), int, cuda\n",
    "        '''\n",
    "        ret = {}\n",
    "        output = self.input_conv(input)\n",
    "        output = self.unet(output)\n",
    "        output = self.output_layer(output)\n",
    "        output_feats = output.features[input_map.long()]\n",
    "\n",
    "        # semantic segmentation\n",
    "        semantic_scores = self.semantic_linear(output_feats)   # (N, nClass), float\n",
    "\n",
    "        semantic_preds = semantic_scores.max(1)[1]    # (N), long\n",
    "\n",
    "        ret['semantic_scores'] = semantic_scores\n",
    "\n",
    "        # center shift vector\n",
    "        # pt_offsets = self.offset_linear(output_feats)  # (N, 3), float32\n",
    "        # ret['pt_offsets'] = pt_offsets\n",
    "        \"\"\" \n",
    "                if(epoch > self.prepare_epochs):\n",
    "\n",
    "            if self.cfg.dataset == 'Synthetic_v3_InstanceSegmentation':\n",
    "                object_idxs = torch.nonzero(semantic_preds > 0).view(-1) # floor idx 0, wall idx 1\n",
    "            else:\n",
    "                raise Exception\n",
    "\n",
    "            # fliter out floor and wall\n",
    "            batch_idxs_ = batch_idxs[object_idxs]\n",
    "            batch_offsets_ = utils.get_batch_offsets(batch_idxs_, input.batch_size)\n",
    "            coords_ = coords[object_idxs]\n",
    "            pt_offsets_ = pt_offsets[object_idxs]  # (N_fg, 3), float32\n",
    "\n",
    "            semantic_preds_cpu = semantic_preds[object_idxs].int().cpu()\n",
    "\n",
    "            idx, start_len = hais_ops.ballquery_batch_p(coords_ + pt_offsets_, \\\n",
    "                batch_idxs_, batch_offsets_, self.point_aggr_radius, self.cluster_shift_meanActive)\n",
    "            \n",
    "            using_set_aggr_in_training = getattr(self.cfg, 'using_set_aggr_in_training', True)\n",
    "            using_set_aggr_in_testing = getattr(self.cfg, 'using_set_aggr_in_testing', True)\n",
    "            using_set_aggr = using_set_aggr_in_training if training_mode == 'train' else using_set_aggr_in_testing\n",
    "\n",
    "            proposals_idx, proposals_offset = hais_ops.hierarchical_aggregation(\n",
    "                semantic_preds_cpu, (coords_ + pt_offsets_).cpu(), idx.cpu(), start_len.cpu(),\n",
    "                batch_idxs_.cpu(), training_mode, using_set_aggr)             \n",
    "\n",
    "            proposals_idx[:, 1] = object_idxs[proposals_idx[:, 1].long()].int()\n",
    "    \n",
    "\n",
    "            # restrict the num of training proposals, avoid OOM\n",
    "            max_proposal_num = getattr(self.cfg, 'max_proposal_num', 200)\n",
    "            if training_mode == 'train' and proposals_offset.shape[0] > max_proposal_num:\n",
    "                proposals_offset = proposals_offset[:max_proposal_num + 1]\n",
    "                proposals_idx = proposals_idx[: proposals_offset[-1]]\n",
    "                assert proposals_idx.shape[0] == proposals_offset[-1]\n",
    "                print('selected proposal num', proposals_offset.shape[0] - 1)\n",
    "\n",
    "            # proposals voxelization again\n",
    "            input_feats, inp_map = self.clusters_voxelization(proposals_idx, proposals_offset, output_feats, coords, self.score_fullscale, self.score_scale, self.score_mode)\n",
    "\n",
    "            # predict instance scores\n",
    "            score = self.intra_ins_unet(input_feats)\n",
    "            score = self.intra_ins_outputlayer(score)\n",
    "            score_feats = score.features[inp_map.long()] # (sumNPoint, C)\n",
    "\n",
    "            # predict mask scores\n",
    "            # first linear than voxel to point,  more efficient  (because voxel num < point num)\n",
    "            mask_scores = self.mask_linear(score.features)\n",
    "            mask_scores = mask_scores[inp_map.long()]\n",
    "\n",
    "            # predict instance scores\n",
    "            if getattr(self.cfg, 'use_mask_filter_score_feature', False)  and \\\n",
    "                    epoch > self.cfg.use_mask_filter_score_feature_start_epoch:\n",
    "                mask_index_select = torch.ones_like(mask_scores)\n",
    "                mask_index_select[torch.sigmoid(mask_scores) < self.cfg.mask_filter_score_feature_thre] = 0.\n",
    "                score_feats = score_feats * mask_index_select\n",
    "            score_feats = hais_ops.roipool(score_feats, proposals_offset.cuda())  # (nProposal, C)\n",
    "            scores = self.score_linear(score_feats)  # (nProposal, 1)\n",
    "            \n",
    "            ret['proposal_scores'] = (scores, proposals_idx, proposals_offset, mask_scores)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "def model_fn_decorator(test=False):\n",
    "    # config\n",
    "    from util.config import cfg\n",
    "\n",
    "    class_weight = torch.FloatTensor(cfg.class_weight).cuda()\n",
    "    semantic_criterion = nn.CrossEntropyLoss(ignore_index=cfg.ignore_label,weight=class_weight).cuda()\n",
    "    score_criterion = nn.BCELoss(reduction='none').cuda()\n",
    "\n",
    "    def test_model_fn(batch, model, epoch):\n",
    "        coords = batch['locs'].cuda()              # (N, 1 + 3), long, cuda, dimension 0 for batch_idx\n",
    "        voxel_coords = batch['voxel_locs'].cuda()  # (M, 1 + 3), long, cuda\n",
    "        p2v_map = batch['p2v_map'].cuda()          # (N), int, cuda\n",
    "        v2p_map = batch['v2p_map'].cuda()          # (M, 1 + maxActive), int, cuda\n",
    "\n",
    "        coords_float = batch['locs_float'].cuda()  # (N, 3), float32, cuda\n",
    "        feats = batch['feats'].cuda()              # (N, C), float32, cuda\n",
    "        batch_offsets = batch['offsets'].cuda()    # (B + 1), int, cuda\n",
    "        spatial_shape = batch['spatial_shape']\n",
    "\n",
    "        if cfg.use_coords:\n",
    "            feats = torch.cat((feats, coords_float), 1)\n",
    "\n",
    "        voxel_feats = hais_ops.voxelization(feats, v2p_map, cfg.mode)  # (M, C), float, cuda\n",
    "\n",
    "        input_ = spconv.SparseConvTensor(voxel_feats, voxel_coords.int(), spatial_shape, cfg.batch_size)\n",
    "\n",
    "        ret = model(input_, p2v_map, coords_float, coords[:, 0].int(), batch_offsets, epoch, 'test')\n",
    "        semantic_scores = ret['semantic_scores']  # (N, nClass) float32, cuda\n",
    "        pt_offsets = ret['pt_offsets']            # (N, 3), float32, cuda\n",
    "\n",
    "        if (epoch > cfg.prepare_epochs):\n",
    "            scores, proposals_idx, proposals_offset, mask_scores = ret['proposal_scores']\n",
    "\n",
    "        # preds\n",
    "        with torch.no_grad():\n",
    "            preds = {}\n",
    "            preds['semantic'] = semantic_scores\n",
    "            preds['pt_offsets'] = pt_offsets\n",
    "            if (epoch > cfg.prepare_epochs):\n",
    "                preds['score'] = scores\n",
    "                preds['proposals'] = (proposals_idx, proposals_offset, mask_scores)\n",
    "\n",
    "        return preds\n",
    "        \n",
    "    def model_fn(batch, model, epoch):\n",
    "        # batch {'locs': locs, 'voxel_locs': voxel_locs, 'p2v_map': p2v_map, 'v2p_map': v2p_map,\n",
    "        # 'locs_float': locs_float, 'feats': feats, 'labels': labels, 'instance_labels': instance_labels,\n",
    "        # 'instance_info': instance_infos, 'instance_pointnum': instance_pointnum,\n",
    "        # 'id': tbl, 'offsets': batch_offsets, 'spatial_shape': spatial_shape}\n",
    "        coords = batch['locs'].cuda()                          # (N, 1 + 3), long, cuda, dimension 0 for batch_idx\n",
    "        voxel_coords = batch['voxel_locs'].cuda()              # (M, 1 + 3), long, cuda\n",
    "        p2v_map = batch['p2v_map'].cuda()                      # (N), int, cuda\n",
    "        v2p_map = batch['v2p_map'].cuda()                      # (M, 1 + maxActive), int, cuda\n",
    "\n",
    "        coords_float = batch['locs_float'].cuda()              # (N, 3), float32, cuda\n",
    "        feats = batch['feats'].cuda()                          # (N, C), float32, cuda\n",
    "        labels = batch['labels'].cuda()                        # (N), long, cuda\n",
    "        instance_labels = batch['instance_labels'].cuda()      # (N), long, cuda, 0~total_nInst, -100\n",
    "\n",
    "        instance_info = batch['instance_info'].cuda()          # (N, 9), float32, cuda, (meanxyz, minxyz, maxxyz)\n",
    "        instance_pointnum = batch['instance_pointnum'].cuda()  # (total_nInst), int, cuda\n",
    "        batch_offsets = batch['offsets'].cuda()                # (B + 1), int, cuda\n",
    "        spatial_shape = batch['spatial_shape']\n",
    "\n",
    "        if cfg.use_coords:\n",
    "            feats = torch.cat((feats, coords_float), 1)\n",
    "\n",
    "        voxel_feats = hais_ops.voxelization(feats, v2p_map, cfg.mode)  # (M, C), float, cuda\n",
    "\n",
    "        input_ = spconv.SparseConvTensor(voxel_feats, voxel_coords.int(), spatial_shape, cfg.batch_size)\n",
    "\n",
    "        ret = model(input_, p2v_map, coords_float, coords[:, 0].int(), batch_offsets, epoch, 'train')\n",
    "        semantic_scores = ret['semantic_scores'] # (N, nClass) float32, cuda\n",
    "        pt_offsets = ret['pt_offsets']           # (N, 3), float32, cuda\n",
    "        \n",
    "        if(epoch > cfg.prepare_epochs):\n",
    "            scores, proposals_idx, proposals_offset, mask_scores = ret['proposal_scores']\n",
    "            # scores: (nProposal, 1) float, cuda\n",
    "            # proposals_idx: (sumNPoint, 2), int, cpu, [:, 0] for cluster_id, [:, 1] for corresponding point idxs in N\n",
    "            # proposals_offset: (nProposal + 1), int, cpu\n",
    "            # mask_scores: (sumNPoint, 1), float, cuda\n",
    "\n",
    "        loss_inp = {}\n",
    "\n",
    "        loss_inp['semantic_scores'] = (semantic_scores, labels)\n",
    "        loss_inp['pt_offsets'] = (pt_offsets, coords_float, instance_info, instance_labels)\n",
    "\n",
    "        if(epoch > cfg.prepare_epochs):\n",
    "            loss_inp['proposal_scores'] = (scores, proposals_idx, proposals_offset, instance_pointnum, mask_scores)\n",
    "\n",
    "        loss, loss_out = loss_fn(loss_inp, epoch)\n",
    "\n",
    "        # accuracy / visual_dict / meter_dict\n",
    "        with torch.no_grad():\n",
    "            preds = {}\n",
    "            preds['semantic'] = semantic_scores\n",
    "            preds['pt_offsets'] = pt_offsets\n",
    "            if(epoch > cfg.prepare_epochs):\n",
    "                preds['score'] = scores\n",
    "                preds['proposals'] = (proposals_idx, proposals_offset)\n",
    "\n",
    "            visual_dict = {}\n",
    "            visual_dict['loss'] = loss\n",
    "            for k, v in loss_out.items():\n",
    "                visual_dict[k] = v[0]\n",
    "\n",
    "            meter_dict = {}\n",
    "            meter_dict['loss'] = (loss.item(), coords.shape[0])\n",
    "            for k, v in loss_out.items():\n",
    "                meter_dict[k] = (float(v[0]), v[1])\n",
    "\n",
    "        return loss, preds, visual_dict, meter_dict\n",
    "\n",
    "\n",
    "    def loss_fn(loss_inp, epoch):\n",
    "\n",
    "        loss_out = {}\n",
    "\n",
    "        '''semantic loss'''\n",
    "        semantic_scores, semantic_labels = loss_inp['semantic_scores']\n",
    "        # semantic_scores: (N, nClass), float32, cuda\n",
    "        # semantic_labels: (N), long, cuda\n",
    "        \n",
    "        semantic_loss = semantic_criterion(semantic_scores, semantic_labels)\n",
    "\n",
    "        loss_out['semantic_loss'] = (semantic_loss, semantic_scores.shape[0])\n",
    "\n",
    "        '''offset loss'''\n",
    "        pt_offsets, coords, instance_info, instance_labels = loss_inp['pt_offsets']\n",
    "        # pt_offsets: (N, 3), float, cuda\n",
    "        # coords: (N, 3), float32\n",
    "        # instance_info: (N, 9), float32 tensor (meanxyz, minxyz, maxxyz)\n",
    "        # instance_labels: (N), long\n",
    "\n",
    "\n",
    "        gt_offsets = instance_info[:, 0:3] - coords   # (N, 3)\n",
    "        pt_diff = pt_offsets - gt_offsets   # (N, 3)\n",
    "        pt_dist = torch.sum(torch.abs(pt_diff), dim=-1)   # (N)       \n",
    "\n",
    "        valid = (instance_labels != cfg.ignore_label).float()\n",
    "\n",
    "        offset_norm_loss = torch.sum(pt_dist * valid) / (torch.sum(valid) + 1e-6)\n",
    "        loss_out['offset_norm_loss'] = (offset_norm_loss, valid.sum())\n",
    "\n",
    "        if (epoch > cfg.prepare_epochs):\n",
    "            '''score and mask loss'''\n",
    "            \n",
    "            scores, proposals_idx, proposals_offset, instance_pointnum, mask_scores = loss_inp['proposal_scores']\n",
    "            # scores: (nProposal, 1), float32\n",
    "            # proposals_idx: (sumNPoint, 2), int, cpu, [:, 0] for cluster_id, [:, 1] for corresponding point idxs in N\n",
    "            # proposals_offset: (nProposal + 1), int, cpu\n",
    "            # instance_pointnum: (total_nInst), int\n",
    "\n",
    "            # get iou and calculate mask label and mask loss\n",
    "            mask_scores_sigmoid = torch.sigmoid(mask_scores)\n",
    "\n",
    "            if getattr(cfg, 'cal_iou_based_on_mask', False) \\\n",
    "                    and (epoch > cfg.cal_iou_based_on_mask_start_epoch):\n",
    "                ious, mask_label =  hais_ops.cal_iou_and_masklabel(proposals_idx[:, 1].cuda(), \\\n",
    "                    proposals_offset.cuda(), instance_labels, instance_pointnum, mask_scores_sigmoid.detach(), 1)\n",
    "            else:\n",
    "                ious, mask_label =  hais_ops.cal_iou_and_masklabel(proposals_idx[:, 1].cuda(), \\\n",
    "                    proposals_offset.cuda(), instance_labels, instance_pointnum, mask_scores_sigmoid.detach(), 0)\n",
    "            # ious: (nProposal, nInstance)\n",
    "            # mask_label: (sumNPoint, 1)\n",
    "\n",
    "            mask_label_weight = (mask_label != -1).float()\n",
    "            mask_label[mask_label==-1.] = 0.5 # any value is ok\n",
    "            mask_loss = torch.nn.functional.binary_cross_entropy(mask_scores_sigmoid, mask_label, weight=mask_label_weight, reduction='none')\n",
    "            mask_loss = mask_loss.mean()\n",
    "            loss_out['mask_loss'] = (mask_loss, mask_label_weight.sum())\n",
    "            gt_ious, _ = ious.max(1)  # gt_ious: (nProposal) float, long\n",
    "            \n",
    "\n",
    "            gt_scores = get_segmented_scores(gt_ious, cfg.fg_thresh, cfg.bg_thresh)\n",
    "\n",
    "            score_loss = score_criterion(torch.sigmoid(scores.view(-1)), gt_scores)\n",
    "            score_loss = score_loss.mean()\n",
    "\n",
    "            loss_out['score_loss'] = (score_loss, gt_ious.shape[0])\n",
    "\n",
    "        '''total loss'''\n",
    "        loss = cfg.loss_weight[0] * semantic_loss + cfg.loss_weight[1] * offset_norm_loss\n",
    "        if(epoch > cfg.prepare_epochs):\n",
    "            loss += (cfg.loss_weight[2] * score_loss)\n",
    "            loss += (cfg.loss_weight[3] * mask_loss)\n",
    "\n",
    "        return loss, loss_out\n",
    "\n",
    "\n",
    "    def get_segmented_scores(scores, fg_thresh=1.0, bg_thresh=0.0):\n",
    "        '''\n",
    "        :param scores: (N), float, 0~1\n",
    "        :return: segmented_scores: (N), float 0~1, >fg_thresh: 1, <bg_thresh: 0, mid: linear\n",
    "        '''\n",
    "        fg_mask = scores > fg_thresh\n",
    "        bg_mask = scores < bg_thresh\n",
    "        interval_mask = (fg_mask == 0) & (bg_mask == 0)\n",
    "\n",
    "        segmented_scores = (fg_mask > 0).float()\n",
    "        k = 1 / (fg_thresh - bg_thresh + 1e-5)\n",
    "        b = bg_thresh / (bg_thresh - fg_thresh + 1e-5)\n",
    "        segmented_scores[interval_mask] = scores[interval_mask] * k + b\n",
    "\n",
    "        return segmented_scores\n",
    "\n",
    "    if test:\n",
    "        fn = test_model_fn\n",
    "    else:\n",
    "        fn = model_fn\n",
    "\n",
    "    return fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HAIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
